{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "tweet_path_list = glob.glob('../data/tweet/*')\n",
    "texts = dict()\n",
    "for path in tweet_path_list[:]:\n",
    "    with open(path) as f:\n",
    "        tweet = json.load(f)\n",
    "    for t in tweet['tweets']:\n",
    "        texts[t['id']] = t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from janome.tokenizer import Tokenizer\n",
    "# from janome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter\n",
    "# from janome.tokenfilter import POSKeepFilter, LowerCaseFilter, ExtractAttributeFilter\n",
    "# from janome.analyzer import Analyzer\n",
    "\n",
    "            \n",
    "# char_filters = [UnicodeNormalizeCharFilter(), # UnicodeをNFKCで正規化\n",
    "#                 RegexReplaceCharFilter('\\d+', '0')] # 数字を全て0に置換\n",
    "\n",
    "# tokenizer = Tokenizer(mmap=True) # NEologdを使う場合、mmap=Trueとする\n",
    "\n",
    "# token_filters = [POSKeepFilter(['名詞', '形容詞', '副詞', '動詞']), # 名詞、形容詞、副詞、動詞のみを抽出する\n",
    "#                  LowerCaseFilter(), # 英字は小文字にする\n",
    "#                  ExtractAttributeFilter('base_form')] # 原型のみを取得する\n",
    "\n",
    "# analyzer = Analyzer(char_filters=char_filters, tokenizer=tokenizer, token_filters=token_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "\n",
    "# ストップワードの取得\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    stopwords = [w for w in response.read().decode().split('\\r\\n') if w != '']\n",
    "\n",
    "stopwords += ['ReTweet', '*']\n",
    "\n",
    "path = \"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    "mecab = MeCab.Tagger(path)\n",
    "\n",
    "#Neologdによるトーカナイザー(リストで返す関数・名詞のみ)\n",
    "def mecab_tokenizer(text):\n",
    "    replaced_text = text.lower()\n",
    "    replaced_text = re.sub(r'[【】]', ' ', replaced_text)       # 【】の除去\n",
    "    replaced_text = re.sub(r'[（）()]', ' ', replaced_text)     # （）の除去\n",
    "    replaced_text = re.sub(r'[［］\\[\\]]', ' ', replaced_text)   # ［］の除去\n",
    "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
    "    replaced_text = re.sub(r'\\d+\\.*\\d*', '', replaced_text) #数字を0にする\n",
    "    replaced_text = re.sub(r'[#＃]', '', replaced_text)\n",
    "\n",
    "    # ノイズとして取り除くパターン\n",
    "    rt = re.compile(r'^RT\\s*')\n",
    "    mention = re.compile(r'\\s*@\\w+:\\s*')\n",
    "    url = re.compile(r'\\s*https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\\s*')\n",
    "\n",
    "    # ノイズ除去\n",
    "    replaced_text = rt.sub('', replaced_text)\n",
    "    replaced_text = mention.sub(' ', replaced_text)\n",
    "    replaced_text = url.sub(' ', replaced_text)\n",
    "\n",
    "    parsed_lines = mecab.parse(replaced_text).split(\"\\n\")[:-2]\n",
    "    \n",
    "    # #表層形を取得\n",
    "    # surfaces = [l.split('\\t')[0] for l in parsed_lines]\n",
    "    #原形を取得\n",
    "    token_list = [l.split(\"\\t\")[1].split(\",\")[6] for l in parsed_lines]\n",
    "    #品詞を取得\n",
    "    pos = [l.split('\\t')[1].split(\",\")[0] for l in parsed_lines]\n",
    "    # 名詞,動詞,形容詞のみに絞り込み\n",
    "    target_pos = [\"名詞\", '形容詞', '副詞', '動詞']\n",
    "    token_list = [t for t, p in zip(token_list, pos) if p in target_pos]\n",
    "    \n",
    "    # stopwordsの除去\n",
    "    token_list = [t for t in token_list if t  not in stopwords]\n",
    "    \n",
    "    # ひらがなのみの単語を除く\n",
    "    kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "    token_list = [t for t in token_list if not kana_re.match(t)]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words count: 800057\n"
     ]
    }
   ],
   "source": [
    "texts_words = {}\n",
    "\n",
    "for k, v in texts.items():\n",
    "    texts_words[k] = mecab_tokenizer(v)\n",
    "\n",
    "print('words count: {}'.format(sum([len(t) for t in texts_words.values()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "\n",
    "\n",
    "# stopwords = []\n",
    "# url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "\n",
    "# # ストップワードの取得\n",
    "# with urllib.request.urlopen(url) as response:\n",
    "#     stopwords = [w for w in response.read().decode().split('\\r\\n') if w != '']\n",
    "\n",
    "# # print('Stopwords: {}'.format(stopwords))\n",
    "# # Stopwords: ['あそこ', 'あたり', 'あちら', 'あっち', 'あと', ...\n",
    "\n",
    "# texts_words = {}\n",
    "\n",
    "# for k, v in texts.items():\n",
    "#     texts_words[k] = [w for w in analyzer.analyze(v)]\n",
    "\n",
    "# print('words count: {}'.format(sum([len(t) for t in texts_words.values()])))\n",
    "# # words count: 28104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim\n",
    "import gensim\n",
    "\n",
    "\n",
    "# 辞書の作成\n",
    "dictionary = gensim.corpora.Dictionary(texts_words.values())\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.4)\n",
    "# 辞書をテキストファイルで保存する場合\n",
    "# dictionary.save_as_text('blog_dictionary.txt')\n",
    "# dictionary = gensim.corpora.Dictionary.load_from_text('blog_dictionary.txt')\n",
    "\n",
    "# print('dictionary: {}'.format(dictionary.token2id))\n",
    "# dictionary: {'行く': 0, 'くる': 1, 'おいしい': 2, '庭': 3, '町屋': 4, '風': 5, '店内': 6, ...\n",
    "\n",
    "# コーパスの作成(ベクトル化)\n",
    "corpus = [dictionary.doc2bow(words) for words in texts_words.values()]\n",
    "# コーパスをテキストファイルで保存する場合\n",
    "# gensim.corpora.MmCorpus.serialize('blog_corpus.mm', corpus)\n",
    "# corpus = gensim.corpora.MmCorpus('blog_corpus.mm')\n",
    "\n",
    "# print('corpus: {}'.format(corpus))\n",
    "# corpus: [[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16298"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDAモデルの構築\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                      num_topics=100, \n",
    "                                      id2word=dictionary, \n",
    "                                      random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(88,\n",
       "  '0.291*\"お願い\" + 0.129*\"セネガル\" + 0.096*\"致す\" + 0.091*\"姫野先輩\" + 0.073*\"可能\" + 0.044*\"悪魔\" + 0.036*\"頂く\" + 0.031*\"完璧\" + 0.024*\"検索\" + 0.024*\"送料\"'),\n",
       " (53,\n",
       "  '0.322*\"交換\" + 0.131*\"人人\" + 0.104*\"レベル\" + 0.076*\"カード\" + 0.060*\"個人的\" + 0.047*\"辛い\" + 0.041*\"地味\" + 0.034*\"量\" + 0.030*\"会える\" + 0.028*\"ドロップ\"'),\n",
       " (9,\n",
       "  '0.340*\"今日\" + 0.132*\"SS\" + 0.098*\"メンテ\" + 0.084*\"貼る\" + 0.059*\"入り\" + 0.050*\"仕事\" + 0.047*\"軸\" + 0.036*\"涙。\" + 0.022*\"掲載\" + 0.018*\"学舎\"'),\n",
       " (95,\n",
       "  '0.191*\"作品\" + 0.137*\"大丈夫\" + 0.118*\"衝撃\" + 0.095*\"姫\" + 0.067*\"作曲\" + 0.067*\"作詞\" + 0.045*\"パイ\" + 0.040*\"最終回\" + 0.030*\"脚気\" + 0.022*\"いつの間にか\"'),\n",
       " (51,\n",
       "  '0.347*\"歌詞\" + 0.111*\"冒険\" + 0.017*\"WILLOW\" + 0.009*\"妖精\" + 0.005*\"theme\" + 0.002*\"再始動\" + 0.002*\"開催決定\" + 0.001*\"女王\" + 0.000*\"公開\" + 0.000*\"チェンソーマン\"'),\n",
       " (68,\n",
       "  '0.396*\"子供\" + 0.153*\"最大\" + 0.099*\"流れ\" + 0.059*\"編\" + 0.052*\"罪\" + 0.026*\"重視\" + 0.015*\"途端\" + 0.009*\"バイト\" + 0.005*\"大型\" + 0.004*\"前者\"'),\n",
       " (64,\n",
       "  '0.131*\"明日\" + 0.093*\"バトル\" + 0.093*\"申し上げる\" + 0.090*\"お知らせ\" + 0.073*\"重要\" + 0.067*\"突然\" + 0.063*\"発言\" + 0.047*\"限\" + 0.039*\"気づく\" + 0.032*\"本気\"'),\n",
       " (97,\n",
       "  '0.603*\"出る\" + 0.088*\"対応\" + 0.067*\"判断\" + 0.065*\"完全\" + 0.031*\"説明\" + 0.031*\"オープン\" + 0.015*\"クーポン\" + 0.013*\"正しい\" + 0.007*\"割引\" + 0.007*\"負担\"'),\n",
       " (48,\n",
       "  '0.170*\"LV\" + 0.166*\"Splatoon\" + 0.148*\"書く\" + 0.103*\"限定\" + 0.061*\"詳細\" + 0.060*\"キャラ\" + 0.059*\"Nintendo\" + 0.027*\"謎\" + 0.024*\"顎口虫症\" + 0.023*\"置く\"'),\n",
       " (71,\n",
       "  '0.201*\"チャンス\" + 0.167*\"ストップ\" + 0.103*\"元気\" + 0.071*\"丸\" + 0.052*\"他人\" + 0.051*\"愛\" + 0.044*\"力こぶ\" + 0.037*\"実現\" + 0.030*\"困る\" + 0.019*\"米\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m topic_counts \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mKyoto\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mGourmet\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mKeitai\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSports\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m texts_words\u001b[39m.\u001b[39mitems():\n\u001b[0;32m----> 9\u001b[0m     category \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m     bow \u001b[39m=\u001b[39m dictionary\u001b[39m.\u001b[39mdoc2bow(v)\n\u001b[1;32m     11\u001b[0m     topics \u001b[39m=\u001b[39m lda\u001b[39m.\u001b[39mget_document_topics(bow)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "topic_counts = {\n",
    "    'Kyoto': [0, 0, 0, 0],\n",
    "    'Gourmet': [0, 0, 0, 0],\n",
    "    'Keitai': [0, 0, 0, 0],\n",
    "    'Sports': [0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "for k, v in texts_words.items():\n",
    "    category = k.split('_')[1]\n",
    "    bow = dictionary.doc2bow(v)\n",
    "    topics = lda.get_document_topics(bow)\n",
    "    \n",
    "    top_topic = sorted(topics, key=lambda topic:topic[1], reverse=True)[0][0]\n",
    "    topic_counts[category][top_topic] += 1\n",
    "\n",
    "print('Topic counts:\\n{}'.format(topic_counts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('lda_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e012de2f7f6e7ae4bed0c80e54c9018dfba9b30bad0a0d12626bd72844933972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
