{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "tweet_path_list = glob.glob('../data/tweet/*')\n",
    "texts = dict()\n",
    "for path in tweet_path_list[:]:\n",
    "    with open(path) as f:\n",
    "        tweet = json.load(f)\n",
    "    for t in tweet['tweets']:\n",
    "        texts[t['id']] = t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from janome.tokenizer import Tokenizer\n",
    "# from janome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter\n",
    "# from janome.tokenfilter import POSKeepFilter, LowerCaseFilter, ExtractAttributeFilter\n",
    "# from janome.analyzer import Analyzer\n",
    "\n",
    "            \n",
    "# char_filters = [UnicodeNormalizeCharFilter(), # UnicodeをNFKCで正規化\n",
    "#                 RegexReplaceCharFilter('\\d+', '0')] # 数字を全て0に置換\n",
    "\n",
    "# tokenizer = Tokenizer(mmap=True) # NEologdを使う場合、mmap=Trueとする\n",
    "\n",
    "# token_filters = [POSKeepFilter(['名詞', '形容詞', '副詞', '動詞']), # 名詞、形容詞、副詞、動詞のみを抽出する\n",
    "#                  LowerCaseFilter(), # 英字は小文字にする\n",
    "#                  ExtractAttributeFilter('base_form')] # 原型のみを取得する\n",
    "\n",
    "# analyzer = Analyzer(char_filters=char_filters, tokenizer=tokenizer, token_filters=token_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "\n",
    "# ストップワードの取得\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    stopwords = [w for w in response.read().decode().split('\\r\\n') if w != '']\n",
    "\n",
    "stopwords += ['ReTweet', '*']\n",
    "\n",
    "path = \"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    "mecab = MeCab.Tagger(path)\n",
    "\n",
    "#Neologdによるトーカナイザー(リストで返す関数・名詞のみ)\n",
    "def mecab_tokenizer(text):\n",
    "    replaced_text = text.lower()\n",
    "    replaced_text = re.sub(r'[【】]', ' ', replaced_text)       # 【】の除去\n",
    "    replaced_text = re.sub(r'[（）()]', ' ', replaced_text)     # （）の除去\n",
    "    replaced_text = re.sub(r'[［］\\[\\]]', ' ', replaced_text)   # ［］の除去\n",
    "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
    "    replaced_text = re.sub(r'\\d+\\.*\\d*', '', replaced_text) #数字を0にする\n",
    "    replaced_text = re.sub(r'[#＃]', '', replaced_text)\n",
    "\n",
    "    # ノイズとして取り除くパターン\n",
    "    rt = re.compile(r'^RT\\s*')\n",
    "    mention = re.compile(r'\\s*@\\w+:\\s*')\n",
    "    url = re.compile(r'\\s*https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\\s*')\n",
    "\n",
    "    # ノイズ除去\n",
    "    replaced_text = rt.sub('', replaced_text)\n",
    "    replaced_text = mention.sub(' ', replaced_text)\n",
    "    replaced_text = url.sub(' ', replaced_text)\n",
    "\n",
    "    parsed_lines = mecab.parse(replaced_text).split(\"\\n\")[:-2]\n",
    "    \n",
    "    # #表層形を取得\n",
    "    # surfaces = [l.split('\\t')[0] for l in parsed_lines]\n",
    "    #原形を取得\n",
    "    token_list = [l.split(\"\\t\")[1].split(\",\")[6] for l in parsed_lines]\n",
    "    #品詞を取得\n",
    "    pos = [l.split('\\t')[1].split(\",\")[0] for l in parsed_lines]\n",
    "    # 名詞,動詞,形容詞のみに絞り込み\n",
    "    target_pos = [\"名詞\", '形容詞', '副詞', '動詞']\n",
    "    token_list = [t for t, p in zip(token_list, pos) if p in target_pos]\n",
    "    \n",
    "    # stopwordsの除去\n",
    "    token_list = [t for t in token_list if t  not in stopwords]\n",
    "    \n",
    "    # ひらがなのみの単語を除く\n",
    "    kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "    token_list = [t for t in token_list if not kana_re.match(t)]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words count: 351262\n"
     ]
    }
   ],
   "source": [
    "texts_words = {}\n",
    "\n",
    "for k, v in texts.items():\n",
    "    texts_words[k] = mecab_tokenizer(v)\n",
    "\n",
    "print('words count: {}'.format(sum([len(t) for t in texts_words.values()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "\n",
    "\n",
    "# stopwords = []\n",
    "# url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "\n",
    "# # ストップワードの取得\n",
    "# with urllib.request.urlopen(url) as response:\n",
    "#     stopwords = [w for w in response.read().decode().split('\\r\\n') if w != '']\n",
    "\n",
    "# # print('Stopwords: {}'.format(stopwords))\n",
    "# # Stopwords: ['あそこ', 'あたり', 'あちら', 'あっち', 'あと', ...\n",
    "\n",
    "# texts_words = {}\n",
    "\n",
    "# for k, v in texts.items():\n",
    "#     texts_words[k] = [w for w in analyzer.analyze(v)]\n",
    "\n",
    "# print('words count: {}'.format(sum([len(t) for t in texts_words.values()])))\n",
    "# # words count: 28104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim\n",
    "import gensim\n",
    "\n",
    "\n",
    "# 辞書の作成\n",
    "dictionary = gensim.corpora.Dictionary(texts_words.values())\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.4)\n",
    "# 辞書をテキストファイルで保存する場合\n",
    "# dictionary.save_as_text('blog_dictionary.txt')\n",
    "# dictionary = gensim.corpora.Dictionary.load_from_text('blog_dictionary.txt')\n",
    "\n",
    "# print('dictionary: {}'.format(dictionary.token2id))\n",
    "# dictionary: {'行く': 0, 'くる': 1, 'おいしい': 2, '庭': 3, '町屋': 4, '風': 5, '店内': 6, ...\n",
    "\n",
    "# コーパスの作成(ベクトル化)\n",
    "corpus = [dictionary.doc2bow(words) for words in texts_words.values()]\n",
    "# コーパスをテキストファイルで保存する場合\n",
    "# gensim.corpora.MmCorpus.serialize('blog_corpus.mm', corpus)\n",
    "# corpus = gensim.corpora.MmCorpus('blog_corpus.mm')\n",
    "\n",
    "# print('corpus: {}'.format(corpus))\n",
    "# corpus: [[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9196"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDAモデルの構築\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                      num_topics=100, \n",
    "                                      id2word=dictionary, \n",
    "                                      random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(52,\n",
       "  '0.320*\"イベント\" + 0.192*\"信じる\" + 0.166*\"エクアドル\" + 0.078*\"狙う\" + 0.044*\"思う\" + 0.030*\"リリース\" + 0.009*\"ST☆RISH\" + 0.006*\"分ける\" + 0.002*\"見る\" + 0.002*\"新宿\"'),\n",
       " (24,\n",
       "  '0.133*\"女性\" + 0.096*\"現実\" + 0.092*\"クラ\" + 0.066*\"恐喝\" + 0.050*\"逮捕\" + 0.049*\"料\" + 0.047*\"売れる\" + 0.044*\"甘い\" + 0.034*\"容疑\" + 0.031*\"被告\"'),\n",
       " (28,\n",
       "  '0.517*\"大学\" + 0.119*\"子供\" + 0.070*\"地元民\" + 0.047*\"昔\" + 0.027*\"お母さん\" + 0.023*\"ネタ\" + 0.020*\"可哀想\" + 0.014*\"乗れる\" + 0.013*\"EN\" + 0.013*\"券\"'),\n",
       " (20,\n",
       "  '0.255*\"可愛い\" + 0.167*\"大好き\" + 0.118*\"楽しい\" + 0.088*\"楽屋\" + 0.080*\"幸せ\" + 0.041*\"続ける\" + 0.034*\"漫画\" + 0.022*\"ストーリー\" + 0.019*\"憧れる\" + 0.018*\"罪\"'),\n",
       " (40,\n",
       "  '0.165*\"取る\" + 0.163*\"戻る\" + 0.111*\"午後\" + 0.094*\"減る\" + 0.089*\"圧倒的\" + 0.063*\"男性\" + 0.055*\"キレ\" + 0.001*\"現在\" + 0.000*\"病院\" + 0.000*\"女性\"'),\n",
       " (8,\n",
       "  '0.505*\"ショック\" + 0.122*\"素晴らしい\" + 0.086*\"全区\" + 0.044*\"デカ\" + 0.037*\"セリフ\" + 0.034*\"刃物\" + 0.023*\"枠\" + 0.022*\"モデル\" + 0.020*\"先輩\" + 0.017*\"引く\"'),\n",
       " (55,\n",
       "  '0.277*\"良い\" + 0.083*\"特急券\" + 0.078*\"乗る\" + 0.067*\"特急\" + 0.066*\"交換\" + 0.043*\"思う\" + 0.039*\"譲\" + 0.035*\"読む\" + 0.034*\"求\" + 0.026*\"車掌\"'),\n",
       " (71,\n",
       "  '0.351*\"楽しみ\" + 0.082*\"作業\" + 0.060*\"食事\" + 0.044*\"牛角\" + 0.039*\"最大\" + 0.031*\"照れ顔\" + 0.029*\"キャンペーン\" + 0.023*\"弱\" + 0.022*\"いい肉の日\" + 0.020*\"券\"'),\n",
       " (69,\n",
       "  '0.325*\"サムライソード\" + 0.105*\"悪魔\" + 0.066*\"星野源のオールナイトニッポン\" + 0.059*\"幽霊\" + 0.055*\"チェンソーマン\" + 0.044*\"嬉し涙\" + 0.038*\"決まる\" + 0.036*\"悲しい\" + 0.029*\"期間限定\" + 0.025*\"沢渡\"'),\n",
       " (93,\n",
       "  '0.458*\"宮台\" + 0.358*\"先生\" + 0.044*\"宮台真司\" + 0.020*\"無理\" + 0.018*\"夫婦\" + 0.013*\"グループA\" + 0.010*\"アート\" + 0.008*\"設定\" + 0.007*\"one\" + 0.007*\"貯める\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m topic_counts \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mKyoto\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mGourmet\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mKeitai\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSports\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m texts_words\u001b[39m.\u001b[39mitems():\n\u001b[0;32m----> 9\u001b[0m     category \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m     bow \u001b[39m=\u001b[39m dictionary\u001b[39m.\u001b[39mdoc2bow(v)\n\u001b[1;32m     11\u001b[0m     topics \u001b[39m=\u001b[39m lda\u001b[39m.\u001b[39mget_document_topics(bow)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "topic_counts = {\n",
    "    'Kyoto': [0, 0, 0, 0],\n",
    "    'Gourmet': [0, 0, 0, 0],\n",
    "    'Keitai': [0, 0, 0, 0],\n",
    "    'Sports': [0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "for k, v in texts_words.items():\n",
    "    category = k.split('_')[1]\n",
    "    bow = dictionary.doc2bow(v)\n",
    "    topics = lda.get_document_topics(bow)\n",
    "    \n",
    "    top_topic = sorted(topics, key=lambda topic:topic[1], reverse=True)[0][0]\n",
    "    topic_counts[category][top_topic] += 1\n",
    "\n",
    "print('Topic counts:\\n{}'.format(topic_counts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('lda_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e012de2f7f6e7ae4bed0c80e54c9018dfba9b30bad0a0d12626bd72844933972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
